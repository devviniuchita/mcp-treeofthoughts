"""Servidor MCP TreeOfThoughts usando fastmcp."""

import asyncio
import atexit
import json
import os
import threading
import traceback
import uuid

from concurrent.futures import Future
from datetime import datetime
from pathlib import Path
from typing import Any
from typing import Coroutine
from typing import Dict
from typing import Literal
from typing import Optional
from typing import Union

from fastmcp import FastMCP
from pydantic import BaseModel


# Importar auth provider nativo do FastMCP se dispon√≠vel
try:
    from fastmcp.server.auth import BearerAuthProvider

    FASTMCP_AUTH_AVAILABLE = True
except ImportError:
    BearerAuthProvider = None
    FASTMCP_AUTH_AVAILABLE = False

from src.graph import create_tot_graph

# Importa√ß√µes do projeto original
from src.models import GraphState
from src.models import RunConfig
from src.models import RunTask
from src.utils.path_mirror import ensure_mirror


# Auth provider simples para tokens n√£o-JWT
from fastmcp.server.middleware import Middleware, MiddlewareContext
from mcp import McpError
from mcp.types import ErrorData

class SimpleTokenAuthProvider:
    """Auth provider simples que valida tokens string contra AUTH_TOKEN env var."""
    
    def __init__(self, token: str):
        self.expected_token = token
        
    def get_middleware(self) -> "SimpleTokenAuthMiddleware":
        """Retorna o middleware de autentica√ß√£o."""
        return SimpleTokenAuthMiddleware(self.expected_token)


class SimpleTokenAuthMiddleware(Middleware):
    """Middleware que valida Authorization: Bearer <token>."""
    
    def __init__(self, expected_token: str):
        self.expected_token = expected_token
        
    async def on_request(self, context: MiddlewareContext, call_next):
        """Validate Bearer token in all requests."""
        
        # Skip validation if no HTTP context available (e.g., stdio transport)
        if not hasattr(context, 'request') or not context.request:
            return await call_next(context)
        
        auth_header = context.request.headers.get("authorization")
        if not auth_header or not auth_header.startswith("Bearer "):
            print(f"üö´ Missing or invalid Authorization header: {auth_header}")
            raise McpError(ErrorData(code=-32001, message="Authorization required"))
            
        token = auth_header.split(" ", 1)[1]
        if token != self.expected_token:
            print(f"üö´ Token mismatch - provided: {token[:10]}..., expected: {self.expected_token[:10]}...")
            raise McpError(ErrorData(code=-32002, message="Invalid token"))
            
        print(f"‚úÖ Authorization successful for token: {token[:10]}...")
        return await call_next(context)
# Configurar autentica√ß√£o se AUTH_TOKEN estiver presente
auth_provider = None
auth_token = os.getenv("AUTH_TOKEN")

if auth_token:
    # Usar auth provider simples que aceita tokens UUID/string
    auth_provider = SimpleTokenAuthProvider(token=auth_token)
    print(
        f"üîê SimpleTokenAuthProvider configurado: length={len(auth_token)} characters"
    )
else:
    print("üîì AUTH_TOKEN n√£o configurado - authentication disabled")

# Inicializar o servidor MCP com auth se configurado
mcp = FastMCP("MCP TreeOfThoughts", auth=auth_provider)

# Armazenamento em mem√≥ria para execu√ß√µes ativas
active_runs: Dict[str, Dict[str, Any]] = {}


# Garantir espelhos de arquivos esperados pelos testes de integra√ß√£o
ensure_mirror(
    [
        (Path(__file__).resolve(), Path("/home/ubuntu/server.py")),
        (
            Path(__file__).resolve().parent / "src" / "nodes.py",
            Path("/home/ubuntu/src/nodes.py"),
        ),
    ]
)


# Gerenciamento de event loop para execu√ß√µes em background compat√≠vel com ambientes de teste


class _BackgroundRunner:
    """Estrutura simples para armazenar loop e thread de execu√ß√£o."""

    loop: Optional[asyncio.AbstractEventLoop] = None
    thread: Optional[threading.Thread] = None


_background_runner = _BackgroundRunner()


def _ensure_background_loop() -> asyncio.AbstractEventLoop:
    """Garante que existe um event loop rodando em thread pr√≥pria."""

    try:
        return asyncio.get_running_loop()
    except RuntimeError:
        pass

    if _background_runner.loop is None or _background_runner.loop.is_closed():
        _background_runner.loop = asyncio.new_event_loop()

        def _run_loop(loop: asyncio.AbstractEventLoop) -> None:
            asyncio.set_event_loop(loop)
            loop.run_forever()

        _background_runner.thread = threading.Thread(
            target=_run_loop, args=(_background_runner.loop,), daemon=True
        )
        _background_runner.thread.start()

    if _background_runner.loop is None:
        msg = "Falha ao inicializar event loop em background."
        raise RuntimeError(msg)

    return _background_runner.loop


def _schedule_background_task(
    coro: Coroutine[Any, Any, Any],
) -> Union[asyncio.Task[Any], Future[Any]]:
    """Agenda corrotina usando o event loop apropriado."""

    try:
        loop = asyncio.get_running_loop()
    except RuntimeError:
        loop = _ensure_background_loop()
        return asyncio.run_coroutine_threadsafe(coro, loop)

    return loop.create_task(coro)


_original_create_task = asyncio.create_task


def _create_task_with_fallback(
    coro: Coroutine[Any, Any, Any], *args: Any, **kwargs: Any
) -> Union[asyncio.Task[Any], Future[Any]]:
    """Substitui asyncio.create_task com suporte a ambientes sem loop ativo."""

    try:
        return _original_create_task(coro, *args, **kwargs)
    except RuntimeError:
        loop = _ensure_background_loop()
        return asyncio.run_coroutine_threadsafe(coro, loop)


asyncio.create_task = _create_task_with_fallback  # type: ignore[assignment]


def _shutdown_background_loop() -> None:
    """Finaliza o loop em background ao encerrar o processo."""

    loop = _background_runner.loop
    thread = _background_runner.thread

    if loop and loop.is_running():
        loop.call_soon_threadsafe(loop.stop)

    if thread and thread.is_alive():
        thread.join(timeout=1)


atexit.register(_shutdown_background_loop)


# Modelos para os tools
class RunRequest(BaseModel):
    task: RunTask
    config: Optional[RunConfig] = None


class RunResponse(BaseModel):
    run_id: str
    status: str


class StatusResponse(BaseModel):
    run_id: str
    status: str
    metrics: Optional[Dict[str, Any]] = None


class TraceResponse(BaseModel):
    run_id: str
    status: str
    result: Optional[Dict[str, Any]] = None
    current_state: Optional[Dict[str, Any]] = None
    message: Optional[str] = None


class StopResponse(BaseModel):
    run_id: str
    status: str
    message: str


def iniciar_processo_tot(
    instrucao: str,
    restricoes: Optional[str] = None,
    task_id: Optional[str] = None,
    max_depth: int = 3,
    branching_factor: int = 2,
    beam_width: int = 2,
    max_nodes: int = 50,
    max_time_seconds: int = 60,
    strategy: str = "beam_search",
) -> str:
    """
    Inicia um novo processo Tree of Thoughts para resolver uma tarefa complexa.

    Retorna o ID da execu√ß√£o para monitoramento.
    """
    try:
        # Gerar ID √∫nico se n√£o fornecido
        run_id = task_id if task_id else str(uuid.uuid4())

        # Verificar se j√° existe uma execu√ß√£o com este ID
        if run_id in active_runs:
            return f"Erro: Execu√ß√£o com ID {run_id} j√° existe."

        # Criar tarefa
        task = RunTask(instruction=instrucao, constraints=restricoes, task_id=run_id)

        # Criar configura√ß√£o com estrat√©gia din√¢mica
        config = RunConfig(
            strategy=strategy,  # Agora usando o par√¢metro strategy
            max_depth=max_depth,
            branching_factor=branching_factor,
            beam_width=beam_width,
            stop_conditions={
                "max_nodes": max_nodes,
                "max_time_seconds": max_time_seconds,
            },
        )

        # Criar evento de cancelamento
        cancellation_event = asyncio.Event()

        # Inicializar estado do grafo
        initial_state = GraphState(run_id=run_id, task=task, config=config)

        # Adicionar evento de cancelamento ao estado (n√£o ser√° serializado)
        initial_state.cancellation_event = cancellation_event

        # Armazenar estado inicial com refer√™ncia ao evento de cancelamento
        active_runs[run_id] = {
            "status": "running",
            "state": initial_state.model_dump(),
            "result": None,
            "start_time": datetime.now().isoformat(),
            "cancellation_event": cancellation_event,
            # Refer√™ncia para cancelamento real
            "task": None,  # Ser√° preenchido com a task asyncio
        }

        # Criar e executar o grafo em background
        tot_graph = create_tot_graph()

        async def _executar_em_background():
            try:
                # Executar o grafo LangGraph
                raw_final_state = await tot_graph.ainvoke(initial_state)

                # Verificar se foi cancelado durante a execu√ß√£o
                if cancellation_event.is_set():
                    active_runs[run_id]["status"] = "cancelled"
                    active_runs[run_id]["end_time"] = datetime.now().isoformat()
                    print(f"Execu√ß√£o {run_id} foi cancelada durante a execu√ß√£o.")
                    return

                # Processar o estado final
                if isinstance(raw_final_state, GraphState):
                    final_state = raw_final_state
                else:
                    try:
                        final_state = GraphState(**raw_final_state)
                    except Exception as e:
                        aviso = (
                            "[AVISO] N√£o foi poss√≠vel converter raw_final_state "
                            f"para GraphState: {e}"
                        )
                        print(aviso)
                        final_state = raw_final_state

                # Atualizar status
                active_runs[run_id]["status"] = "completed"
                active_runs[run_id]["result"] = (
                    final_state.model_dump()
                    if isinstance(final_state, GraphState)
                    else final_state
                )
                active_runs[run_id]["end_time"] = datetime.now().isoformat()

                final_answer = (
                    final_state.final_answer
                    if isinstance(final_state, GraphState)
                    else final_state.get("final_answer", "N/A")
                )
                mensagem_final = (
                    f"Execu√ß√£o {run_id} conclu√≠da. Resposta final: {final_answer}"
                )
                print(mensagem_final)

            except asyncio.CancelledError:
                # Task foi cancelada
                active_runs[run_id]["status"] = "cancelled"
                active_runs[run_id]["end_time"] = datetime.now().isoformat()
                print(f"Execu√ß√£o {run_id} foi cancelada.")
                raise  # Re-raise para finalizar a task corretamente

            except Exception as e:
                active_runs[run_id]["status"] = "failed"
                active_runs[run_id]["error"] = str(e)
                active_runs[run_id]["traceback"] = traceback.format_exc()
                active_runs[run_id]["end_time"] = datetime.now().isoformat()
                print(f"Execu√ß√£o {run_id} falhou com erro: {e}")

        # Executar em background e armazenar refer√™ncia da task
        task_ref = _schedule_background_task(_executar_em_background())
        active_runs[run_id]["task"] = task_ref

        return (
            "Processo Tree of Thoughts iniciado com sucesso. "
            f"ID da execu√ß√£o: {run_id}. Estrat√©gia: {strategy}"
        )

    except Exception as e:
        return f"Erro ao iniciar processo: {str(e)}"


def verificar_status(run_id: str) -> str:
    """
    Verifica o status atual de uma execu√ß√£o Tree of Thoughts.

    Retorna informa√ß√µes sobre o progresso e m√©tricas da execu√ß√£o.
    """
    try:
        run_data = active_runs.get(run_id)
        if not run_data:
            return f"Execu√ß√£o com ID {run_id} n√£o encontrada."

        status = run_data["status"]
        start_time = run_data.get("start_time", "N/A")

        result = f"Status da execu√ß√£o {run_id}: {status}\n"
        result += f"Iniciado em: {start_time}\n"

        if run_data.get("end_time"):
            result += f"Finalizado em: {run_data['end_time']}\n"

        if run_data.get("result") and run_data["result"].get("metrics"):
            metrics = run_data["result"]["metrics"]
            result += "M√©tricas:\n"
            for key, value in metrics.items():
                result += f"  - {key}: {value}\n"

        if run_data.get("error"):
            result += f"Erro: {run_data['error']}\n"

        return result

    except Exception as e:
        return f"Erro ao verificar status: {str(e)}"


def obter_resultado_completo(run_id: str) -> str:
    """
    Obt√©m o resultado completo de uma execu√ß√£o Tree of Thoughts finalizada.

    Retorna a resposta final, trace completo e m√©tricas detalhadas.
    """
    try:
        run_data = active_runs.get(run_id)
        if not run_data:
            return f"Execu√ß√£o com ID {run_id} n√£o encontrada."

        status = run_data["status"]

        if status == "running":
            return (
                f"Execu√ß√£o {run_id} ainda est√° em andamento. "
                "Use verificar_status para acompanhar o progresso."
            )

        if status == "failed":
            error = run_data.get("error", "Erro desconhecido")
            return f"Execu√ß√£o {run_id} falhou com erro: {error}"

        if status == "completed":
            result = run_data.get("result")
            if not result:
                return f"Execu√ß√£o {run_id} conclu√≠da mas sem resultado dispon√≠vel."

            final_answer = result.get("final_answer", "N/A")
            metrics = result.get("metrics", {})

            response = f"Execu√ß√£o {run_id} conclu√≠da com sucesso!\n\n"
            response += f"RESPOSTA FINAL:\n{final_answer}\n\n"

            if metrics:
                response += "M√âTRICAS:\n"
                for key, value in metrics.items():
                    response += f"  - {key}: {value}\n"

            return response

        return f"Status desconhecido para execu√ß√£o {run_id}: {status}"

    except Exception as e:
        return f"Erro ao obter resultado: {str(e)}"


def cancelar_execucao(run_id: str) -> str:
    """
    Cancela uma execu√ß√£o Tree of Thoughts em andamento de forma real e imediata.

    Implementa cancelamento real atrav√©s de asyncio.Event e task.cancel().
    """
    try:
        if run_id not in active_runs:
            return f"Execu√ß√£o com ID {run_id} n√£o encontrada."

        run_data = active_runs[run_id]

        if run_data["status"] != "running":
            return (
                f"Execu√ß√£o {run_id} n√£o est√° em andamento (status: "
                f"{run_data['status']})."
            )

        # Cancelamento real implementado
        cancellation_event = run_data.get("cancellation_event")
        task_ref = run_data.get("task")

        if cancellation_event:
            # Acionar o evento de cancelamento para interromper os n√≥s do grafo
            cancellation_event.set()
            print(f"[CANCEL] Evento de cancelamento acionado para execu√ß√£o {run_id}")

        if task_ref and not task_ref.done():
            # Cancelar a task asyncio diretamente
            task_ref.cancel()
            print(f"[CANCEL] Task asyncio cancelada para execu√ß√£o {run_id}")

        # Atualizar status imediatamente
        active_runs[run_id]["status"] = "cancelled"
        active_runs[run_id]["end_time"] = datetime.now().isoformat()

        return (
            f"Execu√ß√£o {run_id} foi cancelada com sucesso. "
            "O processo foi interrompido imediatamente."
        )

    except Exception as e:
        return f"Erro ao cancelar execu√ß√£o: {str(e)}"


def listar_execucoes() -> str:
    """
    Lista todas as execu√ß√µes Tree of Thoughts (ativas e finalizadas).

    Retorna um resumo de todas as execu√ß√µes armazenadas.
    """
    try:
        if not active_runs:
            return "EXECU√á√ïES TREE OF THOUGHTS:\n\nNenhuma execu√ß√£o encontrada."

        result = "EXECU√á√ïES TREE OF THOUGHTS:\n\n"

        for run_id, run_data in active_runs.items():
            status = run_data["status"]
            start_time = run_data.get("start_time", "N/A")

            result += f"ID: {run_id}\n"
            result += f"  Status: {status}\n"
            result += f"  Iniciado: {start_time}\n"

            if run_data.get("end_time"):
                result += f"  Finalizado: {run_data['end_time']}\n"

            if run_data.get("result") and run_data["result"].get("final_answer"):
                answer_preview = run_data["result"]["final_answer"][:100]
                if len(run_data["result"]["final_answer"]) > 100:
                    answer_preview += "..."
                result += f"  Resposta: {answer_preview}\n"

            result += "\n"

        return result

    except Exception as e:
        return f"Erro ao listar execu√ß√µes: {str(e)}"


def obter_configuracao_padrao() -> str:
    """
    Retorna a configura√ß√£o padr√£o do MCP TreeOfThoughts.
    """
    try:
        # Tentar carregar defaults.json se existir
        defaults_path = Path("defaults.json")
        if defaults_path.exists():
            with open(defaults_path, 'r', encoding='utf-8') as f:
                defaults = json.load(f)
            return json.dumps(defaults, indent=2, ensure_ascii=False)
        else:
            # Configura√ß√£o padr√£o hardcoded
            default_config = {
                "strategy": "beam_search",
                "branching_factor": 3,
                "max_depth": 3,
                "beam_width": 2,
                "propose_temp": 0.7,
                "value_temp": 0.2,
                "use_value_model": False,
                "parallelism": 4,
                "per_node_token_estimate": 150,
                "stop_conditions": {"max_nodes": 50, "max_time_seconds": 60},
                "evaluation_weights": {
                    "progress": 0.4,
                    "promise": 0.3,
                    "confidence": 0.3,
                },
            }
            return json.dumps(default_config, indent=2, ensure_ascii=False)
    except Exception as e:
        return f"Erro ao obter configura√ß√£o padr√£o: {str(e)}"


def obter_informacoes_sistema() -> str:
    """
    Retorna informa√ß√µes sobre o sistema MCP TreeOfThoughts.
    """
    info = """
    MCP TreeOfThoughts - Racioc√≠nio Avan√ßado para LLMs

    Este √© um servidor Model Context Protocol (MCP) que implementa a metodologia
    Tree of Thoughts (ToT) para resolver problemas complexos atrav√©s de explora√ß√£o
    deliberada de m√∫ltiplos caminhos de racioc√≠nio.

    CARACTER√çSTICAS:
    - Racioc√≠nio ToT avan√ßado com explora√ß√£o de m√∫ltiplos caminhos
    - Orquestra√ß√£o inteligente com LangGraph
    - Cache sem√¢ntico de alta performance (FAISS)
    - Compatibilidade com Google Generative AI (Gemini)
    - Estrat√©gias de busca plug√°veis (Beam Search, etc.)
    - Cancelamento de tarefas em tempo real

    FERRAMENTAS DISPON√çVEIS:
    - iniciar_processo_tot: Inicia uma nova execu√ß√£o ToT
    - verificar_status: Verifica o progresso de uma execu√ß√£o
    - obter_resultado_completo: Obt√©m resultados finais
    - cancelar_execucao: Cancela execu√ß√µes em andamento
    - listar_execucoes: Lista todas as execu√ß√µes

    RECURSOS DISPON√çVEIS:
    - config://defaults: Configura√ß√£o padr√£o do sistema
    - info://sobre: Informa√ß√µes sobre o sistema
    """
    return info


# Registrar tools e recursos mantendo as fun√ß√µes originais cham√°veis nos testes
mcp.tool()(iniciar_processo_tot)
mcp.tool()(verificar_status)
mcp.tool()(obter_resultado_completo)
mcp.tool()(cancelar_execucao)
mcp.tool()(listar_execucoes)
mcp.resource("config://defaults")(obter_configuracao_padrao)
mcp.resource("info://sobre")(obter_informacoes_sistema)


if __name__ == "__main__":
    # Configurar vari√°veis de ambiente se necess√°rio
    if os.path.exists(".env"):
        from dotenv import load_dotenv

        load_dotenv()

    transport_env = os.getenv("MCP_TRANSPORT", "stdio").lower()
    transport: Literal["stdio", "http", "sse", "streamable-http"]
    transport_kwargs: Dict[str, Any] = {}

    if transport_env in {"http", "streamable-http", "streamable_http"}:
        host = os.getenv("MCP_HOST", "127.0.0.1")
        port_value = os.getenv("MCP_PORT", "5173")
        path = os.getenv("MCP_PATH")

        try:
            port = int(port_value)
        except (TypeError, ValueError):
            raise ValueError("MCP_PORT deve ser um n√∫mero inteiro v√°lido.") from None

        transport_kwargs["host"] = host
        transport_kwargs["port"] = port

        if path:
            transport_kwargs["path"] = path

        transport = "streamable-http" if transport_env == "streamable_http" else "http"
    elif transport_env == "sse":
        transport = "sse"
    else:
        # Caso o valor seja inv√°lido, volta para stdio para garantir compatibilidade.
        transport = "stdio"

    # Executar o servidor MCP com o transporte configurado (padr√£o: stdio)
    mcp.run(transport=transport, **transport_kwargs)
